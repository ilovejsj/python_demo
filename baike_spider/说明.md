这个脚本是按照慕课网：http://www.imooc.com/learn/563 手打
时间为：2016-12-28
手抄作者：行书
python版本：python2.7
需要安装beautifulsoup（这里不讨论如何安装）

运行文件为spider_main 用python的IDE打开后就能直接运行。
或者用DOS命令切换到spider_main所在的文件夹下输入python spider_main.py
python就能自动运行

spider_main中的if count == 1000:后面的1000是脚本采集的词条数量


spider_main正文中的下列代码的作用是如果采集不到标题（find不到H1）就输出craw failed，
            except:
                print 'craw failed'
如果你想看扩展其他的功能，或者查看具体什么地方错误，可以改成这样
			except IOError:
                print 'craw failed'

问：spider_main中的root_url后面的内容为初始url，怎么获取？
比较简单的方法，点击当前词条中的本词条内链，就能获取。
root_url = 'http://baike.baidu.com/view/1047.htm'

此脚本其他问题：
1.初始词条固定了，不管接下去你把count的数字换成多少，更改前的count数量是多少，那么就有多少内容是重复的。
2.spider_main关闭以后，重新开始，output里是没内容的，除非count对应的数量的词条全部采集后才会写入
3.spider_main运行过程中 只显示craw 数量 :url 没显示标题
4.没有使用jieba等模块，没有语义分析功能，没有广告词提取的功能
5.百度百科内链不太正规，经常是无意义词语也加了内链。如果找相关词条的话，其实更应该采集词条右边的【其他人还看】内的内容。
